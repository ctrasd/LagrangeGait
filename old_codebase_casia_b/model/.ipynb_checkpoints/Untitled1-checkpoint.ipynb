{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hairy-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as tordata\n",
    "from torch.optim import lr_scheduler\n",
    "#from network import TripletLoss, SetNet\n",
    "#from network import vgg_c3d,C3D_VGG_angle,transview\n",
    "from utils import TripletSampler\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, bias=False, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return F.leaky_relu(x, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "class Feed_Forward(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels,dropout=0.5, **kwargs):\n",
    "        super(Feed_Forward, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, hidden_channels, (1,1), bias=False, **kwargs)\n",
    "        self.relu=nn.LeakyReLU(inplace=True)\n",
    "        self.drop=nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, out_channels, (1,1), bias=False, **kwargs)\n",
    "        self.upsample=nn.Conv2d(in_channels, out_channels, (1,1), bias=False, **kwargs)\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "    def forward(self, x):\n",
    "        x_ori=x.clone()\n",
    "        x=self.conv(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.drop(x)\n",
    "        x=self.conv2(x) # n c h w\n",
    "        x=x.permute(0,2,3,1).contiguous()\n",
    "        x=self.norm(x).permute(0,3,1,2).contiguous()\n",
    "\n",
    "        return x+self.upsample(x_ori)\n",
    "\n",
    "class Feed_Forward_ori(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Non_local(nn.Module):\n",
    "    def __init__(self, in_channels, reduc_ratio=4):\n",
    "        super(Non_local, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = in_channels // reduc_ratio\n",
    "\n",
    "        self.g = nn.Conv2d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                      kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.W = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                    kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(self.in_channels),\n",
    "        )\n",
    "        nn.init.constant_(self.W[1].weight, 0.0)\n",
    "        nn.init.constant_(self.W[1].bias, 0.0)\n",
    "\n",
    "        self.theta = nn.Conv2d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.phi = nn.Conv2d(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "                :param x: (b, c, h, w)\n",
    "                :return x: (b, c, h, w)\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        N = f.size(-1)\n",
    "        #f_div_C = f / N\n",
    "        f_div_C=F.softmax(f,-1)\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Attention_ori(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        \n",
    "        # get q,k,v from a single weight matrix\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        #self.to_q=nn.Linear(dim, inner_dim , bias = False)\n",
    "        #self.to_k=nn.Linear(dim, inner_dim , bias = False)\n",
    "        #self.to_v=nn.Linear(dim, inner_dim , bias = False)\n",
    "        #self.to_qkv.apply(weights_init_kaiming)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        #self.to_out.apply(weights_init_kaiming)\n",
    "    def forward(self, x, mask = None):\n",
    "        #print(x.shape)\n",
    "        x_ori=x.clone()\n",
    "        #ipdb.set_trace()\n",
    "        # x:[batch_size, patch_num, pathch_embedding_dim]\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        #print(b, n, _, h)\n",
    "        # get qkv tuple:([batch, patch_num, head_num*head_dim],[...],[...])\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        \n",
    "        # split q,k,v from [batch, patch_num, head_num*head_dim] -> [batch, head_num, patch_num, head_dim]\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "        #print(q.shape,v.shape)\n",
    "        #transpose(k) * q / sqrt(head_dim) -> [batch, head_num, patch_num, patch_num]\n",
    "        #dots = torch.einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        \n",
    "        q=q.contiguous().view(b*h,n,-1)\n",
    "        k=k.contiguous().view(b*h,-1,n).contiguous()\n",
    "        dots=torch.bmm(q,k)\n",
    "        dots=dots.view(b,h,n,n)\n",
    "        \n",
    "        #print(dots.shape)\n",
    "        \n",
    "        \n",
    "        # mask value: -inf\n",
    "        '''\n",
    "        mask_value = -10000000000000\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
    "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "            del mask\n",
    "        '''\n",
    "        # softmax normalization -> attention matrix\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        # value * attention matrix -> output\n",
    "        \n",
    "        attn=attn.view(b*h,n,-1)\n",
    "        v=v.contiguous().view(b*h,n,-1).contiguous()\n",
    "        out=torch.bmm(attn,v)\n",
    "        out=out.view(b,h,n,-1)\n",
    "        \n",
    "        #out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "\n",
    "        # cat all output -> [batch, patch_num, head_num*head_dim]\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        #out = out.view(b,h,n,d).permute(0,2,1,3).contiguous()\n",
    "        #out=out.view(b,n,h*d)\n",
    "        # Linear + Dropout\n",
    "        out =  self.to_out(out)\n",
    "        out =  self.norm(out)\n",
    "        # out: [batch, patch_num, embedding_dim]\n",
    "        return out+x_ori\n",
    "\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        \n",
    "        # get q,k,v from a single weight matrix\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        #self.to_q=nn.Linear(dim, inner_dim , bias = False)\n",
    "        #self.to_k=nn.Linear(dim, inner_dim , bias = False)\n",
    "        #self.to_v=nn.Linear(dim, inner_dim , bias = False)\n",
    "        #self.to_qkv.apply(weights_init_kaiming)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        #self.to_out.apply(weights_init_kaiming)\n",
    "    def forward(self, x, mask = None):\n",
    "        # bt c h w\n",
    "        bt,c,he,wi=x.shape\n",
    "        #print(x.shape)\n",
    "        x_ori=x.clone()\n",
    "        x=x.permute(0,2,3,1).contiguous()\n",
    "        x=x.view(bt,he*wi,c)\n",
    "        #ipdb.set_trace()\n",
    "        # x:[batch_size, patch_num, pathch_embedding_dim]\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        #print(b, n, _, h)\n",
    "        # get qkv tuple:([batch, patch_num, head_num*head_dim],[...],[...])\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        \n",
    "        # split q,k,v from [batch, patch_num, head_num*head_dim] -> [batch, head_num, patch_num, head_dim]\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "        #print(q.shape,v.shape)\n",
    "        #transpose(k) * q / sqrt(head_dim) -> [batch, head_num, patch_num, patch_num]\n",
    "        #dots = torch.einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        \n",
    "        q=q.contiguous().view(b*h,n,-1)\n",
    "        k=k.contiguous().view(b*h,-1,n).contiguous()\n",
    "        dots=torch.bmm(q,k)\n",
    "        dots=dots.view(b,h,n,n)\n",
    "        \n",
    "        #print(dots.shape)\n",
    "        \n",
    "        \n",
    "        # mask value: -inf\n",
    "        '''\n",
    "        mask_value = -10000000000000\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
    "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
    "            mask = mask[:, None, :] * mask[:, :, None]\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "            del mask\n",
    "        '''\n",
    "        # softmax normalization -> attention matrix\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        # value * attention matrix -> output\n",
    "        \n",
    "        attn=attn.view(b*h,n,-1)\n",
    "        v=v.contiguous().view(b*h,n,-1).contiguous()\n",
    "        out=torch.bmm(attn,v)\n",
    "        out=out.view(b,h,n,-1)\n",
    "        \n",
    "        #out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
    "\n",
    "        # cat all output -> [batch, patch_num, head_num*head_dim]\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        #out = out.view(b,h,n,d).permute(0,2,1,3).contiguous()\n",
    "        #out=out.view(b,n,h*d)\n",
    "        # Linear + Dropout\n",
    "        out =  self.to_out(out)\n",
    "        out =  self.norm(out)\n",
    "        out = out.view(bt,he,wi,-1).permute(0,3,1,2).contiguous()\n",
    "        # out: [batch, patch_num, embedding_dim]\n",
    "        return out+x_ori\n",
    "    \n",
    "class SetBlock(nn.Module):\n",
    "    def __init__(self, forward_block, pooling=False):\n",
    "        super(SetBlock, self).__init__()\n",
    "        self.forward_block = forward_block\n",
    "        self.pooling = pooling\n",
    "        if pooling:\n",
    "            self.pool2d = nn.MaxPool2d(2)\n",
    "    def forward(self, x):\n",
    "        n, s, c, h, w = x.size()\n",
    "        x = self.forward_block(x.view(-1,c,h,w))\n",
    "        if type(x)==tuple:\n",
    "            att_map=x[1]\n",
    "            x=x[0]\n",
    "            if self.pooling:\n",
    "                x = self.pool2d(x)\n",
    "            _, c, h, w = x.size()\n",
    "            return x.view(n, s, c, h ,w),att_map\n",
    "        else:\n",
    "            if self.pooling:\n",
    "                x = self.pool2d(x)\n",
    "            _, c, h, w = x.size()\n",
    "            return x.view(n, s, c, h ,w)    \n",
    "\n",
    "\n",
    "class SetBlock_feature(nn.Module):\n",
    "    def __init__(self, forward_block, pooling=False):\n",
    "        super(SetBlock_feature, self).__init__()\n",
    "        self.forward_block = forward_block\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, s,h, c = x.size()\n",
    "        x = self.forward_block(x.view(-1,h,c))\n",
    "        if type(x)==tuple:\n",
    "            att_map=x[1]\n",
    "            x=x[0]\n",
    "            _, h,c = x.size()\n",
    "            return x.view(n, s,h, c),att_map\n",
    "        else:\n",
    "            \n",
    "            _, h,c = x.size()\n",
    "            return x.view(n, s,h, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
