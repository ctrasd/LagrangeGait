{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cellular-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from basic_blocks import SetBlock, BasicConv2d,Non_local,Feed_Forward,Attention,SetBlock_feature,Attention_ori,Feed_Forward_cdim,Feed_Forward_ori\n",
    "\n",
    "def gem(x, p=6.5, eps=1e-6):\n",
    "    # print('x-',x.shape)\n",
    "    # print('xpow-',x.clamp(min=eps).pow(p).shape)\n",
    "    # print(F.avg_pool2d(x.clamp(min=eps).pow(p), (1, x.size(-1))).shape)\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (1, x.size(-1))).pow(1./p)\n",
    "\n",
    "class GeM(nn.Module):\n",
    "\n",
    "    def __init__(self, p=6.5, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('p-',self.p)\n",
    "        return gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n",
    "\n",
    "\n",
    "class BasicConv3d(nn.Module):\n",
    "    def __init__(self, inplanes, planes, dilation=1, bias=False, **kwargs):\n",
    "        super(BasicConv3d, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=(3, 3, 3), bias=bias, dilation=(dilation, 1, 1), padding=(dilation, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x.permute(0,2,1,3,4).contiguous())\n",
    "        out = F.leaky_relu(out, inplace=True)\n",
    "        return out.permute(0,2,1,3,4).contiguous()\n",
    "\n",
    "\n",
    "class mix_mlp(nn.Module):\n",
    "    def __init__(self, inplanes,hidden,planes, thw, norm1=False,norm2=False,short_cut=True,**kwargs):\n",
    "        super(mix_mlp, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        self.fc1=Feed_Forward_ori(thw,thw*2)\n",
    "        self.fc2=Feed_Forward_cdim(inplanes,hidden,planes)\n",
    "        \n",
    "        self.need_norm1=norm1\n",
    "        self.need_norm2=norm2\n",
    "        self.short_cut=short_cut\n",
    "\n",
    "        if norm1:\n",
    "            self.norm1=nn.LayerNorm(thw)\n",
    "        if norm2:\n",
    "            self.norm2=nn.LayerNorm(inplanes)\n",
    "        \n",
    "        if short_cut:\n",
    "            self.downsample=nn.Linear(inplanes,planes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #n*-1 thw c\n",
    "        if self.short_cut:\n",
    "            x_ori=x.clone()\n",
    "\n",
    "        if self.need_norm1:\n",
    "            x=self.norm1(x)\n",
    "        x=x.permute(0,2,1).contiguous()\n",
    "        x=self.fc1(x) # n*-1 c thw\n",
    "        x=x.permute(0,2,1).contiguous() # n*-1 thw c\n",
    "        if self.short_cut:\n",
    "            x=x+x_ori\n",
    "            x_ori=self.downsample(x)\n",
    "        if self.need_norm2:\n",
    "            x=self.norm2(x)\n",
    "        x=self.fc2(x) # n*-1 thw c2\n",
    "        if self.short_cut:\n",
    "            x=x+x_ori\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_patch(x,patch_size_h, patch_size_w, patch_size_t):\n",
    "    n,t,h,w,c=x.shape\n",
    "    x=x.view(n,t//patch_size_t,patch_size_t,h//patch_size_h,patch_size_h,w//patch_size_w,patch_size_w,c)\n",
    "    # n numt pt numh ph numw pw c\n",
    "    x=x.permute(0,1,3,5,2,4,6,7).contiguous().view(n,-1,patch_size_t,patch_size_h,patch_size_w,c)\n",
    "    n,numbin,pt,ph,pw,c=x.shape\n",
    "    return x.view(n,numbin,-1,c)\n",
    "\n",
    "def reserve_patch(x,patch_size_h, patch_size_w, patch_size_t, h, w, t):\n",
    "    #n numt*numh*numw pt*ph*pw c\n",
    "    n,numbin,ppp,c=x.shape\n",
    "    x=x.view(n, t//patch_size_t, h//patch_size_h, w//patch_size_w, patch_size_t, patch_size_h, patch_size_w,c)\n",
    "    x=x.permute(0,1,4,2,5,3,6,7).contiguous()\n",
    "    x=x.view(n,t,h,w,c)\n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "class LocaltemporalAG(nn.Module):\n",
    "    def __init__(self, inplanes, planes, dilation=1, bias=False, **kwargs):\n",
    "        super(LocaltemporalAG, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=(3, 1, 1), stride=(3,1,1), bias=bias,padding=(0, 0, 0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.permute(0,4,1,2,3).contiguous() # b,c,t,h,w\n",
    "        out1 = self.conv1(x)\n",
    "        out = F.leaky_relu(out1, inplace=True)\n",
    "        return out.permute(0,2,3,4,1).contiguous()\n",
    "\n",
    "\n",
    "class BasicConv3d_p(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel=5, bias=False, p=2, FM=False, **kwargs):\n",
    "        super(BasicConv3d_p, self).__init__()\n",
    "        self.p = p\n",
    "        self.fm = FM\n",
    "        self.convdl = nn.Conv3d(inplanes, planes, kernel_size=(kernel, kernel, kernel), bias=bias, padding=((kernel-1)//2, (kernel-1)//2, (kernel-1)//2))\n",
    "        self.convdg = nn.Conv3d(inplanes, planes, kernel_size=(kernel, kernel, kernel), bias=bias, padding=((kernel-1)//2, (kernel-1)//2, (kernel-1)//2))\n",
    "    def forward(self, x):\n",
    "        x=x.permute(0,2,1,3,4).contiguous()\n",
    "        n, c, t, h, w = x.size()\n",
    "        scale = h//self.p\n",
    "        # print('p-',x.shape,n, c, t, h, w,'scale-',scale)\n",
    "        feature = list()\n",
    "        for i in range(self.p):\n",
    "            temp = self.convdl(x[:,:,:,i*scale:(i+1)*scale,:])\n",
    "            # print(temp.shape,i*scale,(i+1)*scale)\n",
    "            feature.append(temp)\n",
    "\n",
    "        outl = torch.cat(feature, 3)\n",
    "        # print('outl-',outl.shape)\n",
    "        outl = F.leaky_relu(outl, inplace=True)\n",
    "\n",
    "        outg = self.convdg(x)\n",
    "        outg = F.leaky_relu(outg, inplace=True)\n",
    "        # print('outg-',outg.shape)\n",
    "        if not self.fm:\n",
    "            # print('1-1')\n",
    "            out = outg + outl\n",
    "        else:\n",
    "            # print('1-2')\n",
    "            out = torch.cat((outg, outl), dim=3)\n",
    "        out=out.permute(0,2,1,3,4).contiguous()\n",
    "        return out\n",
    "\n",
    "\n",
    "class transview_pure(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(transview_pure, self).__init__()\n",
    "        self.batch_frame = None\n",
    "        self.Gem=GeM()\n",
    "\n",
    "        self.patch_size1=4\n",
    "        self.patch_size1_t=3\n",
    "\n",
    "        \n",
    "        self.patch_size2_h=8\n",
    "        self.patch_size2_w=4\n",
    "        self.patch_size2_t=1\n",
    "\n",
    "        self.patch_size3_h_1=4\n",
    "        self.patch_size3_w_1=2\n",
    "        self.patch_size3_t_1=5\n",
    "\n",
    "        self.patch_size3_h_2=2\n",
    "        self.patch_size3_w_2=11\n",
    "        self.patch_size3_t_2=1\n",
    "\n",
    "\n",
    "\n",
    "        self.bin_num=16\n",
    "        self.bin_num2=32\n",
    "        self.bin_num2_w=2\n",
    "        self.bin_num_t=2\n",
    "        _set_in_channels = 1\n",
    "        _set_channels = [32, 64, 128,256]\n",
    "        #self.set_layer1 = SetBlock(BasicConv2d(_set_in_channels, _set_channels[0], 5, padding=2))\n",
    "        #self.set_layer2 = SetBlock(BasicConv2d(_set_channels[0], _set_channels[0], 3, padding=1), True)\n",
    "        self.mlp_layer1=SetBlock_feature(mix_mlp(_set_in_channels,_set_channels[0],_set_channels[0],4*4*3,False,False,False))\n",
    "        self.mlp_layer2=SetBlock_feature(mix_mlp(_set_channels[0],_set_channels[1],_set_channels[1],8*4*1,True,True,True))\n",
    "        \n",
    "\n",
    "\n",
    "        self.tp1=LocaltemporalAG(_set_channels[0],_set_channels[0])\n",
    "\n",
    "\n",
    "        self.mlp_layer3_1=SetBlock_feature(mix_mlp(_set_channels[1],_set_channels[2],_set_channels[1],4*2*5,True,True,True))\n",
    "        self.mlp_layer3_2=SetBlock_feature(mix_mlp(_set_channels[1],_set_channels[2],_set_channels[1],2*11*1,True,True,True))\n",
    "\n",
    "        #self.c3d3=BasicConv3d_p(_set_channels[1], _set_channels[1],FM=True)\n",
    "        #self.non_layer1 = SetBlock(Non_local( _set_channels[1],8))\n",
    "        #self.non_layer2 = SetBlock(Non_local( _set_channels[1],8))\n",
    "\n",
    "        self.non_layer1 = SetBlock(Attention( _set_channels[1],heads=2,dim_head=_set_channels[1]//4,dropout=0.1))\n",
    "        self.non_layer2 = SetBlock(Attention( _set_channels[1],heads=2,dim_head=_set_channels[1]//4,dropout=0.1))\n",
    "\n",
    "\n",
    "        self.fead_forward_layer1=SetBlock(Feed_Forward(_set_channels[1],_set_channels[2],_set_channels[2]))\n",
    "        #self.feedforward_layer1=nn.Sequential(nn.Conv2d(32,32,kernel_size=(1,1),stride=1,padding=0,bias=True))\n",
    "        #self.non_layer3 = SetBlock(Non_local(_set_channels[2],8))\n",
    "        #self.non_layer4 = SetBlock(Non_local(_set_channels[2],8))\n",
    "\n",
    "\n",
    "        self.non_layer3 = SetBlock(Attention( _set_channels[2],heads=2,dim_head=_set_channels[2]//8,dropout=0.1))\n",
    "        self.non_layer4 = SetBlock(Attention( _set_channels[2],heads=2,dim_head=_set_channels[2]//8,dropout=0.1))\n",
    "\n",
    "\n",
    "        self.fead_forward_layer2=SetBlock(Feed_Forward(_set_channels[2],_set_channels[3],_set_channels[3]))\n",
    "        self.pool2d = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.non_layer5 = Attention_ori( _set_channels[3],heads=2,dim_head=_set_channels[3]//4,dropout=0.1)\n",
    "\n",
    "        self.pos_embedding_patch = nn.Parameter(torch.randn(1,1,64,32*2,22))\n",
    "        self.pos_embedding_32 = nn.Parameter(torch.randn(1,1,64,32*2,22))\n",
    "        self.pos_embedding_64 = nn.Parameter(torch.randn(1,1,128,32*2,22))\n",
    "\n",
    "\n",
    "        self.layer_bin=8\n",
    "        self.bin_numgl = [32,64]\n",
    "        \n",
    "        self.fc_bin = nn.Parameter(\n",
    "                nn.init.xavier_uniform_(\n",
    "                    torch.zeros(sum(self.bin_numgl), _set_channels[3], _set_channels[3])))\n",
    "                \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Conv1d)):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant(m.bias.data, 0.0)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "                nn.init.normal(m.weight.data, 1.0, 0.02)\n",
    "                nn.init.constant(m.bias.data, 0.0)\n",
    "\n",
    "    def frame_max(self, x):\n",
    "        if self.batch_frame is None:\n",
    "            return torch.max(x, 1)\n",
    "        else:\n",
    "            _tmp = [\n",
    "                torch.max(x[:, self.batch_frame[i]:self.batch_frame[i + 1], :, :, :], 1)\n",
    "                for i in range(len(self.batch_frame) - 1)\n",
    "                ]\n",
    "            max_list = torch.cat([_tmp[i][0] for i in range(len(_tmp))], 0)\n",
    "            arg_max_list = torch.cat([_tmp[i][1] for i in range(len(_tmp))], 0)\n",
    "            return max_list, arg_max_list\n",
    "\n",
    "    def frame_median(self, x):\n",
    "        if self.batch_frame is None:\n",
    "            return torch.median(x, 1)\n",
    "        else:\n",
    "            _tmp = [\n",
    "                torch.median(x[:, self.batch_frame[i]:self.batch_frame[i + 1], :, :, :], 1)\n",
    "                for i in range(len(self.batch_frame) - 1)\n",
    "                ]\n",
    "            median_list = torch.cat([_tmp[i][0] for i in range(len(_tmp))], 0)\n",
    "            arg_median_list = torch.cat([_tmp[i][1] for i in range(len(_tmp))], 0)\n",
    "            return median_list, arg_median_list\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, silho, batch_frame=None):\n",
    "\n",
    "        n = silho.size(0)\n",
    "        t = silho.size(1)\n",
    "        x=silho\n",
    "        n,t,c,h,w=x.shape\n",
    "        #x = silho.unsqueeze(1)\n",
    "        del silho\n",
    "        if t == 1:\n",
    "            x = x.repeat(1, 6, 1, 1, 1)\n",
    "        elif t == 2 or t==3:\n",
    "            x = x.repeat(1, 3, 1, 1, 1)\n",
    "        #elif t == 3:\n",
    "        #    x = torch.cat((x,x[:,0:3,:,:,:]),dim=1)\n",
    "        elif t<6:\n",
    "            x=x.unsqueeze(1)\n",
    "            x=x.expand(-1,2,-1,-1,-1,-1).contiguous()\n",
    "            #print(x.shape)\n",
    "            x=x.view(n,-1,c,h,w)\n",
    "        x=x[:,:x.shape[1]-x.shape[1]%6]\n",
    "        \n",
    "        \n",
    "        #print(x.shape)\n",
    "        #x = self.set_layer1(x)\n",
    "        # n t c h w\n",
    "        x=x.permute(0,1,3,4,2).contiguous() #n t h w c\n",
    "        b,t,h,w,c=x.shape\n",
    "        x=split_patch(x,self.patch_size1,self.patch_size1,self.patch_size1_t) #n -1 pt*ph*pw c\n",
    "        x=self.mlp_layer1(x)\n",
    "        x=reserve_patch(x,self.patch_size1,self.patch_size1,self.patch_size1_t,h,w,t) #n t h w c\n",
    "\n",
    "        x = self.tp1(x) # n,t/3,32,64,44\n",
    "        \n",
    "        b,t,h,w,c=x.shape\n",
    "        print(x.shape)\n",
    "        x=split_patch(x,self.patch_size2_h,self.patch_size2_w,self.patch_size2_t) # n -1 pt*ph*pw c\n",
    "        x=self.mlp_layer2(x)\n",
    "        x=reserve_patch(x,self.patch_size2_h,self.patch_size2_w,self.patch_size2_t,h,w,t) #n t/3 64 64 44\n",
    "\n",
    "\n",
    "\n",
    "        b,t,h,w,c=x.shape\n",
    "\n",
    "        x=x.permute(0,1,4,2,3).contiguous()\n",
    "        x = x.view(-1,c,h,w)\n",
    "        x = self.pool2d(x)\n",
    "\n",
    "        x = x.view(b,t,c,h//2,w//2)\n",
    "        x=x.permute(0,1,3,4,2).contiguous()\n",
    "\n",
    "        b,t,h,w,c=x.shape\n",
    "\n",
    "        x1=x.clone()\n",
    "        x1=split_patch(x1,self.patch_size3_h_1,self.patch_size3_w_1,self.patch_size3_t_1)\n",
    "        x1=self.mlp_layer3_1(x1)\n",
    "        x1=reserve_patch(x1,self.patch_size3_h_1,self.patch_size3_w_1,self.patch_size3_t_1,h,w,t)\n",
    "\n",
    "        x2=x.clone()\n",
    "        x2=split_patch(x2,self.patch_size3_h_2,self.patch_size3_w_2,self.patch_size3_t_2)\n",
    "        x2=self.mlp_layer3_2(x2)\n",
    "        x2=reserve_patch(X2,self.patch_size3_h_2,self.patch_size3_w_2,self.patch_size3_t_2,h,w,t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        x=torch.cat([x1,x2],2)\n",
    "        x=x.permute(0,1,4,2,3).contiguous()\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x=x+self.pos_embedding_32\n",
    "        n,t,c,h,w=x.shape\n",
    "\n",
    "\n",
    "        win_size=h//self.bin_num\n",
    "        x=x.view(n,t,c,self.bin_num,win_size,w).permute(0,1,3,2,4,5)\\\n",
    "            .contiguous().view(n,t*self.bin_num,c,win_size,w) # n tp c h/p w   \n",
    "\n",
    "        x = self.non_layer1(x) # \n",
    "        #x = self.non_layer2(x) #\n",
    "        x = self.fead_forward_layer1(x)\n",
    "        x = x.view(n,t,self.bin_num,c*2,win_size,w).permute(0,1,3,2,4,5).contiguous().view(n,t,c*2,h,w)\n",
    "\n",
    "\n",
    "        x=x+self.pos_embedding_64\n",
    "        n,t,c,h,w=x.shape\n",
    "        win_size_h=h//self.bin_num2\n",
    "        win_size_w=w//self.bin_num2_w\n",
    "        t_size=t//self.bin_num_t\n",
    "\n",
    "        x_2=x.view(n,self.bin_num_t,t_size,c,self.bin_num2,win_size_h,self.bin_num2_w,win_size_w).permute(0,1,4,6,3,2,5,7)\\\n",
    "            .contiguous().view(n,self.bin_num_t*self.bin_num2*self.bin_num2_w,c,t_size*win_size_h,win_size_w)\n",
    "\n",
    "        #x_2=x.view(n,t,c,self.bin_num,win_size,w).permute(0,1,3,2,4,5)\\\n",
    "        #    .contiguous().view(n,t*self.bin_num,c,win_size,w) # n tp c h/p w   \n",
    "\n",
    "        x_2=self.non_layer3(x_2)\n",
    "        x_2=self.non_layer4(x_2)\n",
    "\n",
    "        x_2 = self.fead_forward_layer2(x_2) \n",
    "        x_2 = x_2.view(n,self.bin_num_t,self.bin_num2,self.bin_num2_w,c*2,t_size,win_size_h,win_size_w).permute(0,1,5,4,2,6,3,7).contiguous()\n",
    "        x_2 = x_2.view(n,t,c*2,h,w)\n",
    "        #x_2 = x_2.view(n,t,self.bin_num,c*2,win_size,w).permute(0,1,3,2,4,5).contiguous().view(n,t,c*2,h,w)\n",
    "        # n t 128 64 22\n",
    "\n",
    "        #x_2=torch.max(x_2,1)[0] # n 128 64 22\n",
    "        #x_2=torch.cat(x_2.mean(1),x.max(1)[0])\n",
    "        x_2=x_2.mean(1)\n",
    "\n",
    "        _, c2d, _, _ = x_2.size()\n",
    "\n",
    "        feature = list()\n",
    "        for num_bin in self.bin_numgl:\n",
    "            z = x_2.view(n, c2d, num_bin, -1).contiguous()\n",
    "            # z1 = z.mean(3) + z.max(3)[0]\n",
    "            # print('z1-',z1.shape)\n",
    "            z2 = self.Gem(z).squeeze(-1)\n",
    "            # print('z2-',z2.shape)\n",
    "            feature.append(z2)\n",
    "        feature = torch.cat(feature, 2).permute(2, 0, 1).contiguous()\n",
    "        #print('feature',feature.shape)\n",
    "        feature = feature.matmul(self.fc_bin) # 96 n 256\n",
    "        #feature = feature.permute(1, 2, 0).contiguous()\n",
    "        #print('feature',feature.shape)\n",
    "\n",
    "        #feature = self.non_layer5(feature)\n",
    "        feature = feature.permute(1, 2, 0).contiguous()\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "breeding-rebel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaitianrui/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:235: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    }
   ],
   "source": [
    "net=transview_pure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "looking-profit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 64, 44, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[64], expected input with shape [*, 64], but got input of size[880, 32, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1440e29d4e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m44\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ee6148c58a5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size2_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size2_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size2_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# n -1 pt*ph*pw c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_layer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreserve_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size2_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size2_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size2_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#n t/3 64 64 44\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/ctr/gait_trans/model/network/basic_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ee6148c58a5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mx_ori\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneed_norm2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# n*-1 thw c2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_cut\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 171\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2200\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2201\u001b[0m         )\n\u001b[0;32m-> 2202\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[64], expected input with shape [*, 64], but got input of size[880, 32, 32]"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1,30,1,64,44)\n",
    "out=net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-physiology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
